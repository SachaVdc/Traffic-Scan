{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49916bc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f309a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f31721",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "154a2497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes de label : [' Green Light', ' Red Light', ' Speed Limit 10', ' Speed Limit 100', ' Speed Limit 110', ' Speed Limit 120', ' Speed Limit 20', ' Speed Limit 30', ' Speed Limit 40', ' Speed Limit 50', ' Speed Limit 60', ' Speed Limit 70', ' Speed Limit 80', ' Speed Limit 90', ' Stop']\n"
     ]
    }
   ],
   "source": [
    "# Charger le CSV\n",
    "csv_path = '_classes2.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Dossier contenant les images\n",
    "image_path = r\"C:\\Users\\malam\\Documents\\2 - ECOLE\\HENALLUX VIRTON\\M1 - INGENIEUR INDUSTRIEL AUTOMATION\\13 - SYSTEMES INTELLIGENTS\\Self-Driving Cars.v6-version-4-prescan-416x416.multiclass\\train\"\n",
    "image_dir = Path(image_path)\n",
    "\n",
    "# Vérifier les colonnes disponibles (les labels)\n",
    "label_columns = df.columns.tolist()[1:]  # On exclut 'filename'\n",
    "\n",
    "print(\"Colonnes de label :\", label_columns)\n",
    "\n",
    "# Vérifie que le fichier existe\n",
    "df['filepath'] = df['filename'].apply(lambda x: str(image_dir / x))\n",
    "df = df[df['filepath'].apply(os.path.exists)]  # Garde uniquement les fichiers valides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda90ec",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42bc2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Fonction pour lire et redimensionner une image\n",
    "def load_and_preprocess_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img / 255.0  # Normalisation\n",
    "\n",
    "# Appliquer à tout le dataset\n",
    "X = np.array([load_and_preprocess_image(path) for path in df['filepath']])\n",
    "y = df[label_columns].values.astype(np.float32)  # Multilabel binaires (0/1)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paramètres d'augmentation\n",
    "ROTATION_RANGE = 30\n",
    "ZOOM_RANGE = 0.2\n",
    "WIDTH_SHIFT = 0.2\n",
    "HEIGHT_SHIFT = 0.2\n",
    "HORIZONTAL_FLIP = True\n",
    "\n",
    "# Augmentation uniquement sur le train\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=ROTATION_RANGE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    width_shift_range=WIDTH_SHIFT,\n",
    "    height_shift_range=HEIGHT_SHIFT,\n",
    "    horizontal_flip=HORIZONTAL_FLIP\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "test_generator = ImageDataGenerator().flow(X_test, y_test, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb9dcd",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb187732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    # Couche d'entrée : spécifie la forme des images (128x128 pixels, 3 canaux RGB)\n",
    "    layers.Input(shape=(*IMG_SIZE, 3)),\n",
    "    \n",
    "    # Première couche de convolution :\n",
    "    # - 32 filtres de taille 3x3\n",
    "    # - Fonction d'activation ReLU\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    # Réduction de dimension par max pooling 2x2\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Deuxième couche de convolution :\n",
    "    # - 64 filtres (plus de filtres pour détecter plus de caractéristiques)\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Troisième couche de convolution :\n",
    "    # - 128 filtres pour des caractéristiques plus complexes\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Aplatissement des données pour les couches denses\n",
    "    layers.Flatten(),\n",
    "    # Couche dense avec 128 neurones \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Dropout de 50% pour éviter le surapprentissage\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # Couche de sortie :\n",
    "    # - Autant de neurones que de classes (len(label_columns))\n",
    "    # - Activation sigmoid pour la classification multi-label\n",
    "    layers.Dense(len(label_columns), activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80444e",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b93fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation et entraînement du modèle\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  # Pour classification multi-label\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a timestamped folder for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f'model_checkpoints_{timestamp}'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    # EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(checkpoint_dir, 'best_model.h5'), \n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "EPOCHS = 50  # Nombre d'epochs pour l'entraînement initial\n",
    "\n",
    "# Augmentation du nombre d'epochs à 15 pour permettre un meilleur apprentissage\n",
    "# - Plus d'epochs = plus d'opportunités pour le modèle d'apprendre\n",
    "# - La validation_data permet de suivre le surapprentissage\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS, \n",
    "    validation_data=test_generator,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadca6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Affichage des courbes\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf2f68",
   "metadata": {},
   "source": [
    "## Save and load the model and try with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Prepare data to save\n",
    "model_data = {\n",
    "    'Date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'Image_Size': f\"{IMG_SIZE[0]}x{IMG_SIZE[1]}\",\n",
    "    'Batch_Size': BATCH_SIZE,\n",
    "    'Epochs': EPOCHS,\n",
    "    'Final_Accuracy': accuracy,\n",
    "    'Final_Loss': loss,\n",
    "    'Model_Architecture': str(model.summary()),\n",
    "    'Data_Augmentation': {\n",
    "        'rotation_range': ROTATION_RANGE,\n",
    "        'zoom_range': ZOOM_RANGE,\n",
    "        'width_shift_range': WIDTH_SHIFT,\n",
    "        'height_shift_range': HEIGHT_SHIFT,\n",
    "        'horizontal_flip': HORIZONTAL_FLIP\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_new = pd.DataFrame([model_data])\n",
    "\n",
    "# Check if file exists\n",
    "csv_filename = 'Model_parameters.csv'\n",
    "if os.path.exists(csv_filename):\n",
    "    # Append to existing file\n",
    "    df_new.to_csv(csv_filename, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # Create new file\n",
    "    df_new.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Model parameters saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_timestamp(model, base_folder=\"model_saves_Panneaux_Scan\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"sign_classifier_{timestamp}.h5\"\n",
    "    \n",
    "    # Full path\n",
    "    model_path = os.path.join(base_folder, model_filename)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(model_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Use the function\n",
    "save_model_with_timestamp(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5fe83-5bb8-43a4-8982-4237f06c3fc2",
   "metadata": {},
   "source": [
    "## Save the model bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d251d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SI_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
