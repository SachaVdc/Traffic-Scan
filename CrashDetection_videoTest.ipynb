{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9da259f",
   "metadata": {},
   "source": [
    "## Step 1 : Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ee26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import keyboard\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80334c31",
   "metadata": {},
   "source": [
    "## Step 2 : Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d071eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLENAME = \"Test_Crash_Table.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(TABLENAME)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only frame columns\n",
    "frame_cols = [f\"frame_{i}\" for i in range(1, 51)]\n",
    "\n",
    "# Select only the frame columns from the DataFrame\n",
    "frame_data = df[frame_cols]\n",
    "\n",
    "# Flatten data into a single list\n",
    "labels = frame_data.values.flatten()\n",
    "\n",
    "print(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1751cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"C:\\\\Users\\\\sacha\\\\OneDrive\\\\Bureau\\\\CrashBest\"\n",
    "BATCH_SIZE = int(len(labels))\n",
    "\n",
    "image_paths = list(Path(IMAGE_PATH).glob(\"*.jpg\"))[:BATCH_SIZE]\n",
    "print(image_paths[0])\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fe4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "# create a dataset class for the crash frames\n",
    "class CrashFrameDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = cv2.cvtColor(cv2.imread(str(image_path)), cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (img_width, img_height))\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Use the existing transform variable defined in a previous cell\n",
    "dataset = CrashFrameDataset(image_paths, labels, transform=transform)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5779398",
   "metadata": {},
   "source": [
    "# Step 3 : Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrashDetection(nn.Module):\n",
    "    def __init__(self, input_channels=3, filter_base=8, input_height=256, input_width=256):\n",
    "        super(CrashDetection, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.filter_base = filter_base\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1_1 = nn.Conv2d(input_channels, filter_base, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(filter_base, filter_base, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(filter_base, filter_base * 2, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(filter_base * 2, filter_base * 2, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(filter_base * 2, filter_base * 4, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(filter_base * 4, filter_base * 4, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Placeholder for output size\n",
    "        self._to_linear = None\n",
    "        self._compute_flattened_size()\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def _compute_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, self.input_channels, self.input_height, self.input_width)\n",
    "            x = F.relu(self.conv1_1(x))\n",
    "            x = F.relu(self.conv1_2(x))\n",
    "            x = self.pool1(x)\n",
    "\n",
    "            x = F.relu(self.conv2_1(x))\n",
    "            x = F.relu(self.conv2_2(x))\n",
    "            x = self.pool2(x)\n",
    "\n",
    "            x = F.relu(self.conv3_1(x))\n",
    "            x = F.relu(self.conv3_2(x))\n",
    "            x = self.pool3(x)\n",
    "\n",
    "            self._to_linear = x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        #print('Output shape of layer 1', x.shape)\n",
    "\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        #print('Output shape of layer 2', x.shape)\n",
    "\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = self.pool3(x)\n",
    "        #print('Output shape of layer 3', x.shape)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #print('Shape required to pass to Linear Layer', x.shape)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CrashDetection().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a24d5",
   "metadata": {},
   "source": [
    "# Step 4 : Load and try the model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "model_paths = glob.glob(\"Models/CarCrashPytorch_*.keras\")\n",
    "model_path = model_paths[len(model_paths) - 1]\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25763b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Set up the figure for displaying images\n",
    "fig, ax = plt.subplots()\n",
    "current_index = 0\n",
    "count_pred_crash = 0\n",
    "count_pred_noCrash = 0\n",
    "count_crash = 0\n",
    "count_noCrash = 0\n",
    "\n",
    "def update_image():\n",
    "    global current_index\n",
    "    global count_pred_crash, count_pred_noCrash, count_crash, count_noCrash\n",
    "    image, label = dataset[current_index]\n",
    "    \n",
    "    # Convert tensor to numpy and transpose to (H, W, C) for imshow\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    else:\n",
    "        image = np.array(image)\n",
    "        \n",
    "    ax.clear()\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "    # Convert image to the format expected by the model\n",
    "    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2).to(device) / 255.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_sigmoid = model(image_tensor)\n",
    "        y_pred = torch.argmax(y_sigmoid, axis=-1).cpu().numpy()\n",
    "\n",
    "    if y_pred == 0:\n",
    "        print(f\"No crash detected\")\n",
    "        count_pred_noCrash += 1\n",
    "    else:\n",
    "        print(f\"Crash detected\")\n",
    "        count_pred_crash += 1\n",
    "    \n",
    "    if label == 0:\n",
    "        count_noCrash += 1\n",
    "    else:\n",
    "        count_crash += 1\n",
    "\n",
    "# loop through the dataset and display images\n",
    "for i in range(len(image_paths)):\n",
    "    current_index = i\n",
    "    update_image()\n",
    "    print(f\"Current index: {current_index + 1}/{len(image_paths)}\")\n",
    "    print(f\"Total no-crash detected: {count_pred_noCrash}\")\n",
    "    print(f\"Total crashes detected: {count_pred_crash}\")\n",
    "    print(f\"Total real no-crash: {count_noCrash}\")\n",
    "    print(f\"Total real crash: {count_crash}\")\n",
    "\n",
    "    if keyboard.is_pressed(\"esc\"): # Exit the loop (Escape Key)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Current index: {current_index + 1}/{len(image_paths)}\")\n",
    "        print(f\"Total no-crash detected: {count_pred_noCrash}\")\n",
    "        print(f\"Total crashes detected: {count_pred_crash}\")\n",
    "        print(f\"Total real no-crash: {count_noCrash}\")\n",
    "        print(f\"Total real crash: {count_crash}\")\n",
    "        \n",
    "        break\n",
    "    \n",
    "    # time.sleep(0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
